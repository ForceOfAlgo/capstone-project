def model_performance_classification_sklearn(model, predictors, target):
    """
    Function to compute different metrics to check classification model performance.

    model: classifier
    predictors: independent variables
    target: dependent variable
    """

    # Predict using the independent variables
    pred = model.predict(predictors)

    acc = accuracy_score(target, pred)  # Compute Accuracy
    recall = recall_score(target, pred, average='weighted')  # Use 'weighted' for multi-class
    precision = precision_score(target, pred, average='weighted')  # Use 'weighted' for multi-class
    f1 = f1_score(target, pred, average='weighted')  # Use 'weighted' for multi-class

    # Compute confusion matrix
    cm = confusion_matrix(target, pred)

    # Compute TP, FP, FN for each class
    class_metrics = []
    for i in range(cm.shape[0]):  # Loop over classes
        TP = cm[i, i]
        FP = cm[:, i].sum() - TP
        FN = cm[i, :].sum() - TP
        class_metrics.append((i, TP, FP, FN))

    # Creating a DataFrame of metrics
    df_perf = pd.DataFrame(
        {
            "Accuracy": [acc],
            "Recall": [recall],
            "Precision": [precision],
            "F1": [f1],
        }
    )

    # Convert per-class metrics to DataFrame
    df_class_metrics = pd.DataFrame(class_metrics, columns=["Class", "TP", "FP", "FN"])

    return df_perf, df_class_metrics
def confusion_matrix_sklearn(model, predictors, target):
    """
    To plot the confusion matrix with percentages for multi-class classification.

    model: classifier
    predictors: independent variables
    target: dependent variable
    """
    y_pred = model.predict(predictors)
    cm = confusion_matrix(target, y_pred)

    # Normalize the confusion matrix by row (true label)
    cm_percent = cm.astype("float") / cm.sum(axis=1)[:, np.newaxis]

    # Format labels to show both count and percentage
    labels = np.array(
        [
            ["{0:0.0f}".format(item) + "\n{0:.1%}".format(item_percent)]
            for item, item_percent in zip(cm.flatten(), cm_percent.flatten())
        ]
    ).reshape(cm.shape)

    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=labels, fmt="", cmap="Blues", xticklabels=np.unique(target), yticklabels=np.unique(target))

    plt.ylabel("True Label")
    plt.xlabel("Predicted Label")
    plt.title("Confusion Matrix")
    plt.show()
# define LabelEncoder
from sklearn.preprocessing import LabelEncoder
# Create a LabelEncoder object
encoder = LabelEncoder()

# Fit the encoder on the 'Type' column of the training data and transform it
X_train['Cards_type'] = encoder.fit_transform(X_train['Cards_type'])


# Transform the 'Type' column in the validation and test data using the trained encoder
X_val['Cards_type'] = encoder.transform(X_val['Cards_type'])
X_test['Cards_type'] = encoder.transform(X_test['Cards_type'])

# Additionally, encode the target variable (y_train, y_val, y_test)
y_train = encoder.fit_transform(y_train)
y_val = encoder.transform(y_val)
y_test = encoder.transform(y_test)
Defining scorer to be used for cross-validation and hyperparameter tuning
scorer = metrics.make_scorer(metrics.recall_score, average = "weighted")
Model building with original data
models = []  # Empty list to store all the models

# Appending models into the list
models.append(("Logistic regression", LogisticRegression(random_state=1)))
models.append(("Bagging", BaggingClassifier(random_state=1)))
models.append(("Random forest", RandomForestClassifier(random_state=1)))
models.append(("GBM", GradientBoostingClassifier(random_state=1)))
models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
models.append(("Xgboost", XGBClassifier(random_state=1, eval_metric="logloss")))
models.append(("dtree", DecisionTreeClassifier(random_state=1)))

results1 = []  # Empty list to store all model's CV scores
names = []  # Empty list to store name of the models


# loop through all models to get the mean cross validated score
print("\n" "Cross-Validation performance on training dataset:" "\n")

for name, model in models:
    kfold = StratifiedKFold(
        n_splits=5, shuffle=True, random_state=1
    )  # Setting number of splits equal to 5
    cv_result = cross_val_score(
        estimator=model, X=X_train, y=y_train, scoring = scorer,cv=kfold
    )
    results1.append(cv_result)
    names.append(name)
    print("{}: {}".format(name, cv_result.mean()))

print("\n" "Validation Performance:" "\n")

for name, model in models:
    model.fit(X_train, y_train)
    scores = recall_score(y_val, model.predict(X_val), average= "weighted")
    print("{}: {}".format(name, scores))
Cross-Validation performance on training dataset:

Logistic regression: 0.1274610232453425
Bagging: 0.8780848389336035
Random forest: 0.8946229901878013
GBM: 0.8952346109829084
Adaboost: 0.27386352976492
Xgboost: 0.9001332057559897
dtree: 0.8486745089210335

Validation Performance:

Logistic regression: 0.11029411764705882
Bagging: 0.8768382352941176
Random forest: 0.9007352941176471
GBM: 0.8933823529411765
Adaboost: 0.32169117647058826
Xgboost: 0.8897058823529411
dtree: 0.8474264705882353
# Plotting boxplots for CV scores of all models defined above
fig = plt.figure(figsize=(10, 7))

fig.suptitle("Algorithm Comparison")
ax = fig.add_subplot(111)

plt.boxplot(results1)
ax.set_xticklabels(names)

plt.show()

Model Building with Oversampled data
# Checking class distribution before oversampling
print("Before OverSampling:")
for label in np.unique(y_train):
    print(f"Class {label}: {sum(y_train == label)}")

# Applying SMOTE for multiclass classification
sm = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=1)  # 'auto' balances all classes
X_train_over, y_train_over = sm.fit_resample(X_train, y_train)

# Checking class distribution after oversampling
print("\nAfter OverSampling:")
for label in np.unique(y_train_over):
    print(f"Class {label}: {sum(y_train_over == label)}")

# Printing new shape of training data
print("\nAfter OverSampling, the shape of X_train: {}".format(X_train_over.shape))
print("After OverSampling, the shape of y_train: {}".format(y_train_over.shape))
Before OverSampling:
Class 0: 86
Class 1: 80
Class 2: 80
Class 3: 74
Class 4: 87
Class 5: 78
Class 6: 90
Class 7: 67
Class 8: 77
Class 9: 87
Class 10: 78
Class 11: 75
Class 12: 75
Class 13: 86
Class 14: 84
Class 15: 95
Class 16: 83
Class 17: 77
Class 18: 79
Class 19: 94

After OverSampling:
Class 0: 95
Class 1: 95
Class 2: 95
Class 3: 95
Class 4: 95
Class 5: 95
Class 6: 95
Class 7: 95
Class 8: 95
Class 9: 95
Class 10: 95
Class 11: 95
Class 12: 95
Class 13: 95
Class 14: 95
Class 15: 95
Class 16: 95
Class 17: 95
Class 18: 95
Class 19: 95

After OverSampling, the shape of X_train: (1900, 5)
After OverSampling, the shape of y_train: (1900,)
# Empty list to store all the models
models = []

# Appending models into the list
models.append(("dtree", DecisionTreeClassifier(random_state=1)))
models.append(("Logistic regression", LogisticRegression(random_state=1)))
models.append(("Bagging", BaggingClassifier(random_state=1)))
models.append(("Random forest", RandomForestClassifier(random_state=1)))
models.append(("GBM", GradientBoostingClassifier(random_state=1)))
models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
models.append(("Xgboost", XGBClassifier(random_state=1)))

results1 = []  # Empty list to store all model's CV scores
names = []  # Empty list to store names of the models

# Loop through all models to get the mean cross-validated score
print("\n Cross-Validation performance on training dataset:\n")

for name, model in models:
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)  # Setting number of splits equal to 5
    cv_result = cross_val_score(estimator=model, X=X_train_over, y=y_train_over, scoring='accuracy', cv=kfold)
    results1.append(cv_result)
    names.append(name)
    print("{}: {}".format(name, cv_result.mean()))

print("\n Validation Performance: \n")

for name, model in models:
    model.fit(X_train_over, y_train_over)  # Train the model on the oversampled training data
    scores = recall_score(y_val, model.predict(X_val), average= "weighted")  # Evaluate the model on the validation data
    print("{}: {}".format(name, scores))
 Cross-Validation performance on training dataset:

dtree: 0.8699999999999999
Logistic regression: 0.12473684210526317
Bagging: 0.8915789473684212
Random forest: 0.9078947368421053
GBM: 0.906842105263158
Adaboost: 0.2731578947368421
Xgboost: 0.9105263157894739

 Validation Performance: 

dtree: 0.8198529411764706
Logistic regression: 0.11213235294117647
Bagging: 0.8860294117647058
Random forest: 0.9191176470588235
GBM: 0.8805147058823529
Adaboost: 0.18198529411764705
Xgboost: 0.9080882352941176
# Plotting boxplots for CV scores of all models defined above
fig = plt.figure(figsize=(10, 7))

fig.suptitle("Algorithm Comparison")
ax = fig.add_subplot(111)

plt.boxplot(results1)
ax.set_xticklabels(names)

plt.show()

Model Building with Undersampled data
# Checking class distribution before undersampling
print("Before UnderSampling:")
for label in np.unique(y_train):
    print(f"Class {label}: {sum(y_train == label)}")

# Random undersampler for under-sampling the data (balance all classes)
rus = RandomUnderSampler(sampling_strategy='auto', random_state=1)
X_train_un, y_train_un = rus.fit_resample(X_train, y_train)

# Checking class distribution after undersampling
print("After UnderSampling:")
for label in np.unique(y_train_un):
    print(f"Class {label}: {sum(y_train_un == label)}")

print("\nAfter UnderSampling, the shape of train_X:", X_train_un.shape)
print("After UnderSampling, the shape of train_y:", y_train_un.shape)
Before UnderSampling:
Class 0: 86
Class 1: 80
Class 2: 80
Class 3: 74
Class 4: 87
Class 5: 78
Class 6: 90
Class 7: 67
Class 8: 77
Class 9: 87
Class 10: 78
Class 11: 75
Class 12: 75
Class 13: 86
Class 14: 84
Class 15: 95
Class 16: 83
Class 17: 77
Class 18: 79
Class 19: 94
After UnderSampling:
Class 0: 67
Class 1: 67
Class 2: 67
Class 3: 67
Class 4: 67
Class 5: 67
Class 6: 67
Class 7: 67
Class 8: 67
Class 9: 67
Class 10: 67
Class 11: 67
Class 12: 67
Class 13: 67
Class 14: 67
Class 15: 67
Class 16: 67
Class 17: 67
Class 18: 67
Class 19: 67

After UnderSampling, the shape of train_X: (1340, 5)
After UnderSampling, the shape of train_y: (1340,)
# Empty list to store all the models
models = []

# Appending models into the list
models.append(("dtree", DecisionTreeClassifier(random_state=1)))
models.append(("Logistic regression", LogisticRegression(random_state=1)))
models.append(("Bagging", BaggingClassifier(random_state=1)))
models.append(("Random forest", RandomForestClassifier(random_state=1)))
models.append(("GBM", GradientBoostingClassifier(random_state=1)))
models.append(("Adaboost", AdaBoostClassifier(random_state=1)))
models.append(("Xgboost", XGBClassifier(random_state=1)))

results1 = []  # Empty list to store all model's CV scores
names = []  # Empty list to store names of the models

# Loop through all models to get the mean cross-validated score
print("\n Cross-Validation performance on training dataset:\n")

for name, model in models:
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)  # Setting number of splits equal to 5
    cv_result = cross_val_score(estimator=model, X=X_train_un, y=y_train_un, scoring='accuracy', cv=kfold)
    results1.append(cv_result)
    names.append(name)
    print("{}: {}".format(name, cv_result.mean()))

print("\n Validation Performance: \n")

for name, model in models:
    model.fit(X_train_un, y_train_un)  # Train the model on the oversampled training data
    scores = recall_score(y_val, model.predict(X_val), average= "weighted")  # Evaluate the model on the validation data
    print("{}: {}".format(name, scores))
 Cross-Validation performance on training dataset:

dtree: 0.8485074626865672
Logistic regression: 0.12761194029850748
Bagging: 0.8701492537313433
Random forest: 0.8940298507462687
GBM: 0.882089552238806
Adaboost: 0.24626865671641793
Xgboost: 0.8858208955223882

 Validation Performance: 

dtree: 0.8216911764705882
Logistic regression: 0.10661764705882353
Bagging: 0.8584558823529411
Random forest: 0.8915441176470589
GBM: 0.8621323529411765
Adaboost: 0.2977941176470588
Xgboost: 0.8786764705882353
# Plotting boxplots for CV scores of all models defined above
fig = plt.figure(figsize=(10, 7))

fig.suptitle("Algorithm Comparison")
ax = fig.add_subplot(111)

plt.boxplot(results1)
ax.set_xticklabels(names)

plt.show()

HyperparameterTuning
Final Models for Hyperparameter Tuning 🥇 1. Random Forest

Best validation performance across all datasets.

Very stable (minimal overfitting).

Top performer especially on oversampled data (0.919 validation score).

🥈 2. XGBoost

Very close to Random Forest.

Sometimes slightly overfits but still very strong.

Performs better with oversampling than with undersampling.

🥉 3. Bagging

Consistent good performer (better than Decision Trees, Logistic Regression, and AdaBoost).

Not as powerful as RF/XGB, but solid backup model.

Sample tuning models
Sample tuning method for Random Forest with original data
GridSearchCV

%%time

# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn import metrics

# Defining model
model = RandomForestClassifier(random_state=1)

# Correct Parameter grid for Random Forest
param_grid = {
    'n_estimators': [150, 200, 250],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']  # for better tuning
}

# Scorer
scorer = metrics.make_scorer(metrics.recall_score, average='macro')  # macro because it's multiclass

# Calling GridSearchCV
grid_cv = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    verbose=2
)

# Fitting
grid_cv.fit(X_train, y_train)

# Results
print("Best parameters are {} with CV score={:.4f}".format(grid_cv.best_params_, grid_cv.best_score_))
Fitting 5 folds for each of 162 candidates, totalling 810 fits
Best parameters are {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200} with CV score=0.9005
CPU times: user 6.99 s, sys: 975 ms, total: 7.97 s
Wall time: 8min 52s
# building model with best parameters
rf_tuned1 = RandomForestClassifier(
    max_depth=20,
    max_features='sqrt',
    min_samples_leaf=1,
    min_samples_split=5,
    n_estimators=200,

)

# Fit the model on training data
rf_tuned1.fit(X_train, y_train)

RandomForestClassifier
?i
RandomForestClassifier(max_depth=20, min_samples_split=5, n_estimators=200)
# Calculating different metrics on train set
rf_grid_train = model_performance_classification_sklearn(
    rf_tuned1, X_train, y_train
)
print("Training performance:")
rf_grid_train
Training performance:
(   Accuracy  Recall  Precision    F1
 0     0.993   0.993      0.993 0.993,
     Class  TP  FP  FN
 0       0  86   1   0
 1       1  80   1   0
 2       2  80   1   0
 3       3  69   0   5
 4       4  86   0   1
 5       5  78   0   0
 6       6  89   6   1
 7       7  67   0   0
 8       8  77   0   0
 9       9  86   0   1
 10     10  77   1   1
 11     11  74   0   1
 12     12  75   0   0
 13     13  86   1   0
 14     14  84   0   0
 15     15  94   0   1
 16     16  83   0   0
 17     17  76   0   1
 18     18  79   0   0
 19     19  94   1   0)
# Calculating different metrics on validation set
rf_grid_val = model_performance_classification_sklearn(rf_tuned1, X_val, y_val)
print("Validation performance:")
rf_grid_val
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.901   0.901      0.907 0.900,
     Class  TP  FP  FN
 0       0  20   8   7
 1       1  24   6   2
 2       2  26   1   0
 3       3  21   3  15
 4       4  25   2   1
 5       5  35   0   0
 6       6  16  11   5
 7       7  33   2   6
 8       8  33   0   0
 9       9  20   7   5
 10     10  29   3   3
 11     11  22   6   0
 12     12  23   0   0
 13     13  23   2   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   0   1
 17     17  22   1   8
 18     18  29   0   0
 19     19  20   1   1)
✅ Training performance is very high (~99%)

✅ Validation performance is still strong (~90%)

➡️ Interpretation:

Your model learned the training data very well, and it's generalizing decently to unseen validation data. There is no extreme overfitting, which is a good sign.

RandomizedSearchCV

%%time

# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import RandomizedSearchCV
# from sklearn.metrics import make_scorer, recall_score

# Define the model
model = RandomForestClassifier(random_state=1)

# Define parameter distribution
param_dist = {
    'n_estimators': [100, 150, 200],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# Define the scorer
scorer = make_scorer(recall_score, average='macro')

# Define RandomizedSearchCV
randomized_cv = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=10,
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    random_state=1,
    verbose=2
)

# Fit RandomizedSearchCV
randomized_cv.fit(X_train, y_train)

# Best parameters and best score
print("Best parameters are {} with CV score={:.4f}:".format(
    randomized_cv.best_params_, randomized_cv.best_score_))
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters are {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30} with CV score=0.8977:
CPU times: user 1.51 s, sys: 127 ms, total: 1.63 s
Wall time: 38.1 s
# building model with best parameters
rf_tuned2 = RandomForestClassifier(
    max_depth=30,
    max_features='sqrt',
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=200,

)

# Fit the model on training data
rf_tuned2.fit(X_train, y_train)

RandomForestClassifier
?i
RandomForestClassifier(max_depth=30, n_estimators=200)
# Calculating different metrics on train set
rf_random_train = model_performance_classification_sklearn(
    rf_tuned2, X_train, y_train
)
print("Training performance:")
rf_random_train
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  86   0   0
 1       1  80   0   0
 2       2  80   0   0
 3       3  74   0   0
 4       4  87   0   0
 5       5  78   0   0
 6       6  90   0   0
 7       7  67   0   0
 8       8  77   0   0
 9       9  87   0   0
 10     10  78   0   0
 11     11  75   0   0
 12     12  75   0   0
 13     13  86   0   0
 14     14  84   0   0
 15     15  95   0   0
 16     16  83   0   0
 17     17  77   0   0
 18     18  79   0   0
 19     19  94   0   0)
# Calculating different metrics on validation set
rf_random_val = model_performance_classification_sklearn(rf_tuned2, X_val, y_val)
print("Validation performance:")
rf_random_val
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.897   0.897      0.902 0.897,
     Class  TP  FP  FN
 0       0  19   8   8
 1       1  23   5   3
 2       2  26   1   0
 3       3  24   3  12
 4       4  25   3   1
 5       5  35   0   0
 6       6  15   9   6
 7       7  33   2   6
 8       8  33   0   0
 9       9  19   9   6
 10     10  28   3   4
 11     11  21   7   1
 12     12  23   0   0
 13     13  23   2   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   1   1
 17     17  22   1   8
 18     18  29   0   0
 19     19  21   1   0)
| Metric | Training | Validation | |:---|:---| | Accuracy | 100% | 89.7% | | Recall (macro avg) | 100% | 89.7% | | Precision (macro avg) | 100% | 90.2% | | F1 Score (macro avg) | 100% | 89.7% |

✅ Training is perfect (as expected for Random Forest on training).

✅ Validation is still very strong (~89.7%).

➡️ Interpretation:

Your model memorized the training data (classic Random Forest behavior).

Generalization to unseen validation data is still very good, just slightly below GridSearchCV (which had ~90.1%).

Sample tuning method for XGBoost with original data
GridSearchCV

%%time

# Defining model for multiclass classification
model = XGBClassifier(random_state=1, eval_metric='mlogloss', objective='multi:softmax', num_class=3)

# Parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [150, 200, 250],
    'learning_rate': [0.1, 0.2],
    'gamma': [0, 3, 5],
    'subsample': [0.8, 0.9],
    'colsample_bytree': [0.8, 1.0],  # Adding colsample_bytree for better tuning
    'max_depth': [3, 5, 7]  # Including max_depth for better control
}

# Type of scoring used to compare parameter combinations
scorer = metrics.make_scorer(metrics.recall_score, average='macro')  # Changed to macro for multiclass

# Calling GridSearchCV
grid_cv = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scorer, cv=5, n_jobs=-1, verbose=2)

# Fitting GridSearchCV
grid_cv.fit(X_train, y_train)

print("Best parameters are {} with CV score={}".format(grid_cv.best_params_, grid_cv.best_score_))
Fitting 5 folds for each of 216 candidates, totalling 1080 fits
Best parameters are {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.9} with CV score=0.9028953100133441
CPU times: user 9.53 s, sys: 1.29 s, total: 10.8 s
Wall time: 11min 6s
# building model with best parameters
xgb_tuned1 = XGBClassifier(
    random_state=1,
    colsample_bytree=1.0,
    gamma=0,
    learning_rate=0.2,
    max_depth=3,
    n_estimators=200,
    subsample=0.9

)

# Fit the model on training data
xgb_tuned1.fit(X_train, y_train)

XGBClassifier
?i
XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=1.0, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=0, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.2, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=200,
              n_jobs=None, num_parallel_tree=None, ...)
# Calculating different metrics on train set
xgboost_grid_train = model_performance_classification_sklearn(
    xgb_tuned1, X_train, y_train
)
print("Training performance:")
xgboost_grid_train
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  86   0   0
 1       1  80   0   0
 2       2  80   0   0
 3       3  74   0   0
 4       4  87   0   0
 5       5  78   0   0
 6       6  90   0   0
 7       7  67   0   0
 8       8  77   0   0
 9       9  87   0   0
 10     10  78   0   0
 11     11  75   0   0
 12     12  75   0   0
 13     13  86   0   0
 14     14  84   0   0
 15     15  95   0   0
 16     16  83   0   0
 17     17  77   0   0
 18     18  79   0   0
 19     19  94   0   0)
# Calculating different metrics on validation set
xgboost_grid_val = model_performance_classification_sklearn(xgb_tuned1, X_val, y_val)
print("Validation performance:")
xgboost_grid_val
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.903   0.903      0.908 0.902,
     Class  TP  FP  FN
 0       0  23  11   4
 1       1  22   2   4
 2       2  26   0   0
 3       3  22   7  14
 4       4  25   2   1
 5       5  35   1   0
 6       6  14   9   7
 7       7  37   2   2
 8       8  33   0   0
 9       9  20   5   5
 10     10  32   2   0
 11     11  18   7   4
 12     12  23   1   0
 13     13  23   3   0
 14     14  21   0   0
 15     15  25   0   0
 16     16  23   0   1
 17     17  24   0   6
 18     18  24   0   5
 19     19  21   1   0)
Training Performance : Perfect (overfitting — memorized training data)

Validation Performance :Strong (90%+ accurate, minor class-specific errors)

Overall Conclusion : Good model, a bit overfit — maybe can regularize slightly more for robustness.

RandomizedSearchCV

%%time

from sklearn.metrics import make_scorer, recall_score
# from xgboost import XGBClassifier
# from sklearn.model_selection import RandomizedSearchCV

# Defining the model
model = XGBClassifier(random_state=1, eval_metric='mlogloss', tree_method='hist')

# Parameter grid
param_grid = {
    'n_estimators': [150, 200, 250],
    'scale_pos_weight': [1, 2, 5],
    'learning_rate': [0.1, 0.2],
    'gamma': [0, 3, 5],
    'subsample': [0.8, 0.9],
    'colsample_bytree': [0.8, 1.0],
    'max_depth': [3, 5, 7]
}

# Defining scorer
scorer = make_scorer(recall_score, average='macro')

# Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=20,  # More iterations for better chance
    n_jobs=-1,
    scoring=scorer,
    cv=5,
    random_state=1,
    verbose=2
)

# Fitting
randomized_cv.fit(X_train, y_train)

# Output
print("Best parameters are {} with CV score={:.4f}".format(
    randomized_cv.best_params_, randomized_cv.best_score_))
Fitting 5 folds for each of 20 candidates, totalling 100 fits
Best parameters are {'subsample': 0.8, 'scale_pos_weight': 1, 'n_estimators': 250, 'max_depth': 3, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.8} with CV score=0.8980
CPU times: user 2.66 s, sys: 236 ms, total: 2.9 s
Wall time: 1min 5s
# building model with best parameters
xgb_tuned2 = XGBClassifier(
    random_state=1,
    subsample=0.8,
    n_estimators=250,
    learning_rate=0.2,
    gamma=0,
    scale_pos_weight=1,
    max_depth=3,
    colsample_bytree=0.8,

)

# Fit the model on training data
xgb_tuned2.fit(X_train, y_train)

XGBClassifier
?i
XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.8, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=0, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.2, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=250,
              n_jobs=None, num_parallel_tree=None, ...)
# Calculating different metrics on training set
xgboost_random_train = model_performance_classification_sklearn(
    xgb_tuned2, X_train, y_train
)
print("Training performance:")
xgboost_random_train
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  86   0   0
 1       1  80   0   0
 2       2  80   0   0
 3       3  74   0   0
 4       4  87   0   0
 5       5  78   0   0
 6       6  90   0   0
 7       7  67   0   0
 8       8  77   0   0
 9       9  87   0   0
 10     10  78   0   0
 11     11  75   0   0
 12     12  75   0   0
 13     13  86   0   0
 14     14  84   0   0
 15     15  95   0   0
 16     16  83   0   0
 17     17  77   0   0
 18     18  79   0   0
 19     19  94   0   0)
# Calculating different metrics on validation set
xgboost_random_val = model_performance_classification_sklearn(xgb_tuned2, X_val, y_val)
print("Validation performance:")
xgboost_random_val
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.899   0.899      0.902 0.898,
     Class  TP  FP  FN
 0       0  23  10   4
 1       1  21   3   5
 2       2  26   0   0
 3       3  20   8  16
 4       4  25   4   1
 5       5  35   0   0
 6       6  15   8   6
 7       7  37   2   2
 8       8  33   0   0
 9       9  20   6   5
 10     10  32   2   0
 11     11  18   6   4
 12     12  23   1   0
 13     13  23   3   0
 14     14  21   0   0
 15     15  25   0   0
 16     16  23   0   1
 17     17  24   1   6
 18     18  25   0   4
 19     19  20   1   1)
✅ Training: Your XGBoost model learned perfectly on training set → (classic for tree ensembles).

✅ Validation: Your model generalizes really well → ~90% accuracy on unseen data is excellent.

❗ Note: Some classes like 3, 6, 9, 11, 17 could use a bit more help (they have slightly higher FN and FP), but it’s not alarming.

Sample tuning method for Bagging with original data
GridSearchCV

# from sklearn.ensemble import BaggingClassifier
# from sklearn.tree import DecisionTreeClassifier
# from sklearn.model_selection import GridSearchCV
# from sklearn import metrics

# Define the model
base_estimator = DecisionTreeClassifier(random_state=1)
model = BaggingClassifier(estimator=base_estimator, random_state=1)

# Define parameter grid
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_samples': [0.7, 0.8, 1.0],
    'max_features': [0.7, 0.8, 1.0],
    'bootstrap': [True, False],
    'estimator__max_depth': [3, 5, 7],
}

# Define scorer
scorer = metrics.make_scorer(metrics.recall_score, average='macro')

# Create GridSearchCV
grid_cv = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scorer, cv=5, n_jobs=-1, verbose=2)

# Fit
grid_cv.fit(X_train, y_train)

# Results
print("Best parameters are {} with CV score={}".format(grid_cv.best_params_, grid_cv.best_score_))
Fitting 5 folds for each of 162 candidates, totalling 810 fits
Best parameters are {'bootstrap': False, 'estimator__max_depth': 7, 'max_features': 0.7, 'max_samples': 0.8, 'n_estimators': 150} with CV score=0.8098546507671894
# building model with best parameters
bagging_tuned1 = BaggingClassifier(
    n_estimators=150,
    max_samples=0.8,
    max_features=0.7,
    bootstrap=False,
    random_state=1
)

# Fit the model on training data
bagging_tuned1.fit(X_train, y_train)

BaggingClassifier
?i
BaggingClassifier(bootstrap=False, max_features=0.7, max_samples=0.8,
                  n_estimators=150, random_state=1)
# Calculating different metrics on training set
bagging_grid_train = model_performance_classification_sklearn(
    bagging_tuned1, X_train, y_train
)
print("Training performance:")
bagging_grid_train
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  86   0   0
 1       1  80   0   0
 2       2  80   0   0
 3       3  74   0   0
 4       4  87   0   0
 5       5  78   0   0
 6       6  90   0   0
 7       7  67   0   0
 8       8  77   0   0
 9       9  87   0   0
 10     10  78   0   0
 11     11  75   0   0
 12     12  75   0   0
 13     13  86   0   0
 14     14  84   0   0
 15     15  95   0   0
 16     16  83   0   0
 17     17  77   0   0
 18     18  79   0   0
 19     19  94   0   0)
# Calculating different metrics on validation set
bagging_grid_val = model_performance_classification_sklearn(bagging_tuned1, X_val, y_val)
print("Validation performance:")
bagging_grid_val
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.860   0.860      0.860 0.858,
     Class  TP  FP  FN
 0       0  13  12  14
 1       1  20   3   6
 2       2  26   1   0
 3       3  18   7  18
 4       4  24   3   2
 5       5  35   0   0
 6       6  15   7   6
 7       7  37   2   2
 8       8  33   0   0
 9       9  19  12   6
 10     10  27   6   5
 11     11  18   8   4
 12     12  21   2   2
 13     13  23   2   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   2   1
 17     17  22   3   8
 18     18  29   0   0
 19     19  19   5   2)
A decent generalization.

But noticeably lower than training (1.0 vs 0.86) → classic overfitting behavior.

Bagging is still performing good (86% is solid for multiclass, many classes!).

RandomizedSearchCV

# from sklearn.model_selection import RandomizedSearchCV

# Define parameter distribution
param_dist = {
    'n_estimators': [50, 75, 100, 125, 150],
    'max_samples': [0.5, 0.7, 0.9, 1.0],
    'max_features': [0.5, 0.7, 0.9, 1.0],
    'bootstrap': [True, False],
    'bootstrap_features': [True, False]
}

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=20,  # You can increase if needed
    cv=5,
    scoring=scorer,
    n_jobs=-1,
    verbose=2,
    random_state=42
)

# Fit the RandomizedSearch
random_search.fit(X_train, y_train)

# Print best parameters
print(f"Best parameters are {random_search.best_params_} with CV score = {random_search.best_score_:.4f}")
Fitting 5 folds for each of 20 candidates, totalling 100 fits
Best parameters are {'n_estimators': 100, 'max_samples': 1.0, 'max_features': 1.0, 'bootstrap_features': True, 'bootstrap': True} with CV score = 0.8988
# building model with best parameters
bagging_tuned2 = BaggingClassifier(
    n_estimators=100,
    max_samples=1.0,
    max_features=1.0,
    bootstrap=True,
    bootstrap_features=True,

)

# Fit the model on training data
bagging_tuned2.fit(X_train, y_train)

BaggingClassifier
?i
BaggingClassifier(bootstrap_features=True, n_estimators=100)
# Calculating different metrics on training set
bagging_random_train = model_performance_classification_sklearn(
    bagging_tuned2, X_train, y_train
)
print("Training performance:")
bagging_random_train
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  86   0   0
 1       1  80   0   0
 2       2  80   0   0
 3       3  74   0   0
 4       4  87   0   0
 5       5  78   0   0
 6       6  90   0   0
 7       7  67   0   0
 8       8  77   0   0
 9       9  87   0   0
 10     10  78   0   0
 11     11  75   0   0
 12     12  75   0   0
 13     13  86   0   0
 14     14  84   0   0
 15     15  95   0   0
 16     16  83   0   0
 17     17  77   0   0
 18     18  79   0   0
 19     19  94   0   0)
# Calculating different metrics on validation set
bagging_random_val = model_performance_classification_sklearn(
bagging_tuned2, X_val, y_val
)

print("Validation performance:")
bagging_random_val
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.879   0.879      0.884 0.878,
     Class  TP  FP  FN
 0       0  19  13   8
 1       1  21   6   5
 2       2  26   1   0
 3       3  21   4  15
 4       4  24   1   2
 5       5  35   0   0
 6       6  15   9   6
 7       7  36   2   3
 8       8  33   0   0
 9       9  18   9   7
 10     10  27   4   5
 11     11  19   6   3
 12     12  23   1   0
 13     13  23   4   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   2   1
 17     17  21   2   9
 18     18  27   0   2
 19     19  21   1   0)
✔️ Randomized SearchCV for Bagging worked very well.

✔️ Much better generalization than without tuning.

✔️ The slight drop from 1.0 (train) to 0.88 (val) is expected — and healthy!

✔️ No signs of heavy class imbalance in validation confusion numbers.

Sample tuning method for Random Forest , XGBoost & Bagging with oversampled data
Random Forest
GridSearchCV

%%time


# Defining model
model = RandomForestClassifier(random_state=1)

# More regularized Parameter grid for Random Forest (to avoid overfitting)
param_grid = {
    'n_estimators': [100, 150, 200],         # moderate number of trees
    'max_depth': [5, 10, 15],                # limit depth (shallow trees help generalize)
    'min_samples_split': [5, 10, 15],        # require more samples to split (prevent small splits)
    'min_samples_leaf': [2, 4, 6],           # each leaf has more samples (prevent memorization)
    'max_features': ['sqrt', 'log2'],        # fewer features per split → more randomness
    'bootstrap': [True]                      # bootstrap=True generally reduces variance
}

# Scorer
scorer = metrics.make_scorer(metrics.recall_score, average='macro')  # macro because it's multiclass

# Calling GridSearchCV
grid_cv = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    verbose=2
)

# Fitting
grid_cv.fit(X_train_over, y_train_over)  # <-- IMPORTANT: oversampled data

# Results
print("\nBest parameters are {} with CV score={:.4f}".format(grid_cv.best_params_, grid_cv.best_score_))
Fitting 5 folds for each of 162 candidates, totalling 810 fits

Best parameters are {'bootstrap': True, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 150} with CV score=0.8968
CPU times: user 5.59 s, sys: 780 ms, total: 6.37 s
Wall time: 6min 40s
# building model with best parameters
rf_tuned3 = RandomForestClassifier(
    bootstrap=True,
    max_depth=15,
    max_features='sqrt',
    min_samples_leaf=2,
    min_samples_split=5,
    n_estimators=150,

)

# Fit the model on training data
rf_tuned3.fit(X_train_over, y_train_over)

RandomForestClassifier
?i
RandomForestClassifier(max_depth=15, min_samples_leaf=2, min_samples_split=5,
                       n_estimators=150)
# Calculating different metrics on train set
rf_grid_train_over = model_performance_classification_sklearn(
    rf_tuned3, X_train_over, y_train_over
)
print("Training performance:")
rf_grid_train_over
Training performance:
(   Accuracy  Recall  Precision    F1
 0     0.980   0.980      0.980 0.980,
     Class  TP  FP  FN
 0       0  92   5   3
 1       1  95   2   0
 2       2  95   1   0
 3       3  88   0   7
 4       4  90   2   5
 5       5  95   0   0
 6       6  92   6   3
 7       7  93   0   2
 8       8  95   0   0
 9       9  91   6   4
 10     10  92   2   3
 11     11  93   8   2
 12     12  94   1   1
 13     13  95   2   0
 14     14  95   0   0
 15     15  94   1   1
 16     16  93   0   2
 17     17  92   0   3
 18     18  95   0   0
 19     19  93   2   2)
# Calculating different metrics on validation set
rf_grid_val_over = model_performance_classification_sklearn(rf_tuned3, X_val, y_val)
print("Validation performance:")
rf_grid_val_over
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.888   0.888      0.895 0.888,
     Class  TP  FP  FN
 0       0  19  12   8
 1       1  24   7   2
 2       2  26   1   0
 3       3  22   5  14
 4       4  25   1   1
 5       5  35   0   0
 6       6  14   8   7
 7       7  34   2   5
 8       8  33   0   0
 9       9  20   4   5
 10     10  27   4   5
 11     11  21  10   1
 12     12  23   1   0
 13     13  23   3   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   0   1
 17     17  21   1   9
 18     18  28   0   1
 19     19  19   1   2)
RandomizedSearchCV

%%time

# Define the model
model = RandomForestClassifier(random_state=1)

# Define parameter distribution
param_dist = {
    'n_estimators': [100, 150, 200],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# Define the scorer
scorer = make_scorer(recall_score, average='macro')

# Define RandomizedSearchCV
randomized_cv = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=10,
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    random_state=1,
    verbose=2
)

# Fit RandomizedSearchCV
randomized_cv.fit(X_train_over, y_train_over)

# Best parameters and best score
print("Best parameters are {} with CV score={:.4f}:".format(
    randomized_cv.best_params_, randomized_cv.best_score_))
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters are {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30} with CV score=0.9058:
CPU times: user 1.45 s, sys: 147 ms, total: 1.6 s
Wall time: 38.1 s
# building model with best parameters
rf_tuned4 = RandomForestClassifier(
    max_depth=30,
    max_features='sqrt',
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=200,

)

# Fit the model on training data
rf_tuned4.fit(X_train_over, y_train_over)

RandomForestClassifier
?i
RandomForestClassifier(max_depth=30, n_estimators=200)
# Calculating different metrics on train set
rf_random_train_over = model_performance_classification_sklearn(
    rf_tuned4, X_train_over, y_train_over
)
print("Training performance:")
rf_random_train_over
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  95   0   0
 1       1  95   0   0
 2       2  95   0   0
 3       3  95   0   0
 4       4  95   0   0
 5       5  95   0   0
 6       6  95   0   0
 7       7  95   0   0
 8       8  95   0   0
 9       9  95   0   0
 10     10  95   0   0
 11     11  95   0   0
 12     12  95   0   0
 13     13  95   0   0
 14     14  95   0   0
 15     15  95   0   0
 16     16  95   0   0
 17     17  95   0   0
 18     18  95   0   0
 19     19  95   0   0)
# Calculating different metrics on validation set
rf_random_val_over = model_performance_classification_sklearn(rf_tuned4, X_val, y_val)
print("Validation performance:")
rf_random_val_over
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.906   0.906      0.912 0.906,
     Class  TP  FP  FN
 0       0  21   6   6
 1       1  24   6   2
 2       2  26   1   0
 3       3  24   3  12
 4       4  25   2   1
 5       5  34   0   1
 6       6  16   9   5
 7       7  34   2   5
 8       8  33   0   0
 9       9  22   8   3
 10     10  27   2   5
 11     11  22   5   0
 12     12  23   0   0
 13     13  23   3   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   0   1
 17     17  22   1   8
 18     18  27   1   2
 19     19  21   1   0)
Xgboost
GridSearchCV

%%time

# Defining model
model = XGBClassifier(random_state=1, eval_metric='mlogloss')

# Parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [150, 200, 250],
    'learning_rate': [0.1, 0.2],
    'gamma': [0, 3, 5],
    'subsample': [0.8, 0.9]
}

# Define scoring metric (macro recall for multiclass)
scorer = make_scorer(recall_score, average="macro")

# Calling GridSearchCV
grid_cv = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    verbose=2
)

# Fitting parameters in GridSearchCV (using oversampled data)
grid_cv.fit(X_train_over, y_train_over)

# Print best parameters and best score
print("Best parameters are {} with CV score={}:" .format(grid_cv.best_params_, grid_cv.best_score_))
Fitting 5 folds for each of 36 candidates, totalling 180 fits
Best parameters are {'gamma': 0, 'learning_rate': 0.2, 'n_estimators': 150, 'subsample': 0.9} with CV score=0.9073684210526315:
CPU times: user 3.25 s, sys: 350 ms, total: 3.6 s
Wall time: 2min 6s
# building model with best parameters
xgb_tuned3 = XGBClassifier(
    random_state=1,
    subsample=0.9,
    n_estimators=150,
    learning_rate=0.2,
    gamma=0,

)

# Fit the model on training data
xgb_tuned3.fit(X_train_over, y_train_over)

XGBClassifier
?i
XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=0, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.2, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=150,
              n_jobs=None, num_parallel_tree=None, ...)
# Calculating different metrics on train set
xgboost_grid_train_over = model_performance_classification_sklearn(
    xgb_tuned3, X_train_over, y_train_over
)
print("Training performance:")
xgboost_grid_train_over
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  95   0   0
 1       1  95   0   0
 2       2  95   0   0
 3       3  95   0   0
 4       4  95   0   0
 5       5  95   0   0
 6       6  95   0   0
 7       7  95   0   0
 8       8  95   0   0
 9       9  95   0   0
 10     10  95   0   0
 11     11  95   0   0
 12     12  95   0   0
 13     13  95   0   0
 14     14  95   0   0
 15     15  95   0   0
 16     16  95   0   0
 17     17  95   0   0
 18     18  95   0   0
 19     19  95   0   0)
# Calculating different metrics on validation set
xgboost_grid_val_over = model_performance_classification_sklearn(xgb_tuned3, X_val, y_val)
print("Validation performance:")
xgboost_grid_val_over
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.904   0.904      0.909 0.904,
     Class  TP  FP  FN
 0       0  23   9   4
 1       1  23   5   3
 2       2  26   0   0
 3       3  22   6  14
 4       4  26   2   0
 5       5  34   0   1
 6       6  16   8   5
 7       7  36   3   3
 8       8  33   0   0
 9       9  21   5   4
 10     10  30   2   2
 11     11  18   7   4
 12     12  23   1   0
 13     13  23   2   0
 14     14  21   0   0
 15     15  25   0   0
 16     16  23   0   1
 17     17  23   0   7
 18     18  25   1   4
 19     19  21   1   0)
RandomizedSearchCV

%%time


# Defining the model
model = XGBClassifier(random_state=1, eval_metric='mlogloss')

# Parameter grid to pass in RandomizedSearchCV
param_grid = {
    'n_estimators': [150, 200, 250],
    'scale_pos_weight': [1, 2, 5],  # Adjusted for multiclass
    'learning_rate': [0.1, 0.2],
    'gamma': [0, 3, 5],
    'subsample': [0.8, 0.9]
}

# Defining the scorer (macro recall for multiclass)
scorer = make_scorer(recall_score, average='macro')

# Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=10,
    n_jobs=-1,
    scoring=scorer,
    cv=5,
    random_state=1,
    verbose=2
)

# Fitting parameters in RandomizedSearchCV
randomized_cv.fit(X_train_over, y_train_over)

# Printing best parameters and best score
print("Best parameters are {} with CV score={:.4f}:".format(
    randomized_cv.best_params_, randomized_cv.best_score_))
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters are {'subsample': 0.9, 'scale_pos_weight': 1, 'n_estimators': 150, 'learning_rate': 0.2, 'gamma': 3} with CV score=0.8547:
CPU times: user 1.34 s, sys: 71.4 ms, total: 1.41 s
Wall time: 28.3 s
# building model with best parameters
xgb_tuned4 = XGBClassifier(
    random_state=1,
    subsample=0.9,
    n_estimators=150,
    learning_rate=0.2,
    gamma=3,
    scale_pos_weight=1,
)

# Fit the model on training data
xgb_tuned4.fit(X_train_over, y_train_over)

XGBClassifier
?i
XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=3, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.2, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=150,
              n_jobs=None, num_parallel_tree=None, ...)
# Calculating different metrics on train set
xgboost_random_train_over = model_performance_classification_sklearn(
    xgb_tuned4, X_train_over, y_train_over
)
print("Training performance:")
xgboost_random_train_over
Training performance:
(   Accuracy  Recall  Precision    F1
 0     0.926   0.926      0.926 0.926,
     Class  TP  FP  FN
 0       0  74  22  21
 1       1  91   6   4
 2       2  95   1   0
 3       3  69  16  26
 4       4  83  16  12
 5       5  95   2   0
 6       6  79  18  16
 7       7  90   1   5
 8       8  95   0   0
 9       9  78  11  17
 10     10  86   8   9
 11     11  89  19   6
 12     12  93   1   2
 13     13  95   3   0
 14     14  95   1   0
 15     15  94   3   1
 16     16  91   0   4
 17     17  84   3  11
 18     18  92   1   3
 19     19  91   9   4)
# Calculating different metrics on validation set
xgboost_random_val_over = model_performance_classification_sklearn(xgb_tuned4, X_val, y_val)
print("Validation performance:")
xgboost_random_val_over
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.807   0.807      0.821 0.807,
     Class  TP  FP  FN
 0       0  14  19  13
 1       1  14   3  12
 2       2  26   2   0
 3       3  15  11  21
 4       4  22   5   4
 5       5  34   1   1
 6       6  15  13   6
 7       7  31   6   8
 8       8  32   0   1
 9       9  19   5   6
 10     10  23   9   9
 11     11  16  17   6
 12     12  23   1   0
 13     13  23   8   0
 14     14  21   0   0
 15     15  25   1   0
 16     16  23   0   1
 17     17  20   2  10
 18     18  23   1   6
 19     19  20   1   1)
Bagging
GridSearchCV for Bagging (Oversampled Data)

%%time

# Defining Bagging model
model = BaggingClassifier(random_state=1)

# Optimized parameter grid for Bagging
param_grid = {
    'n_estimators': [50, 100, 150],        # number of base estimators
    'max_samples': [0.7, 0.8, 1.0],        # % of samples for each estimator
    'max_features': [0.7, 0.8, 1.0],       # % of features for each estimator
    'bootstrap': [True],                   # bootstrap sampling
    'bootstrap_features': [False]          # keep features fixed
}

# Macro recall for balanced evaluation
scorer = metrics.make_scorer(metrics.recall_score, average="macro")

# Optimized GridSearchCV
grid_cv = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring=scorer,
    cv=3,                     # Reduced cv to 3 for speed
    n_jobs=-1,
    verbose=2
)

# Fitting on oversampled data
grid_cv.fit(X_train_over, y_train_over)

# Output best parameters and score
print("Best parameters are {} with CV score={}".format(grid_cv.best_params_, grid_cv.best_score_))
Fitting 3 folds for each of 27 candidates, totalling 81 fits
Best parameters are {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 0.8, 'n_estimators': 150} with CV score=0.9036626344086022
CPU times: user 2.34 s, sys: 105 ms, total: 2.45 s
Wall time: 54.9 s
# building model with best parameters
bagging_tuned3 = BaggingClassifier(
    n_estimators=150,
    max_samples=0.8,
    max_features=0.8,
    bootstrap=True,
    bootstrap_features=False,
)
# Fit the model on training data
bagging_tuned3.fit(X_train_over, y_train_over)

BaggingClassifier
?i
BaggingClassifier(max_features=0.8, max_samples=0.8, n_estimators=150)
# Calculating different metrics on train set
bagging_grid_train_over = model_performance_classification_sklearn(
    bagging_tuned3, X_train_over, y_train_over
)
print("Training performance:")
bagging_grid_train_over
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  95   0   0
 1       1  95   0   0
 2       2  95   0   0
 3       3  95   0   0
 4       4  95   0   0
 5       5  95   0   0
 6       6  95   0   0
 7       7  95   0   0
 8       8  95   0   0
 9       9  95   0   0
 10     10  95   0   0
 11     11  95   0   0
 12     12  95   0   0
 13     13  95   0   0
 14     14  95   0   0
 15     15  95   0   0
 16     16  95   0   0
 17     17  95   0   0
 18     18  95   0   0
 19     19  95   0   0)
# Calculating different metrics on validation set
bagging_grid_val_over = model_performance_classification_sklearn(bagging_tuned3, X_val, y_val)
print("Validation performance:")
bagging_grid_val_over
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.884   0.884      0.893 0.884,
     Class  TP  FP  FN
 0       0  18   9   9
 1       1  23   7   3
 2       2  26   1   0
 3       3  21   4  15
 4       4  25   1   1
 5       5  35   0   0
 6       6  16  10   5
 7       7  35   2   4
 8       8  33   0   0
 9       9  20  11   5
 10     10  27   2   5
 11     11  20   5   2
 12     12  23   1   0
 13     13  23   6   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   1   1
 17     17  22   1   8
 18     18  25   0   4
 19     19  20   1   1)
RandomizedSearchCV for Bagging (Oversampled Data)

%%time

# Defining Bagging model
model = BaggingClassifier(random_state=1)

# Parameter grid for RandomizedSearchCV
param_grid = {
    'n_estimators': [50, 100, 150, 200],      # number of base estimators
    'max_samples': [0.7, 0.8, 0.9, 1.0],      # % of samples
    'max_features': [0.7, 0.8, 0.9, 1.0],     # % of features
    'bootstrap': [True, False],               # bootstrap or not
    'bootstrap_features': [False, True]       # bootstrap features or not
}

# Scoring metric (macro recall)
scorer = metrics.make_scorer(metrics.recall_score, average="macro")

# Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=10,  # number of random combinations
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    random_state=1,
    verbose=2
)

# Fitting on oversampled data
randomized_cv.fit(X_train_over, y_train_over)

# Printing best parameters and score
print("Best parameters are {} with CV score={}".format(randomized_cv.best_params_, randomized_cv.best_score_))
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters are {'n_estimators': 50, 'max_samples': 0.7, 'max_features': 0.9, 'bootstrap_features': False, 'bootstrap': False} with CV score=0.9052631578947368
CPU times: user 1.07 s, sys: 99.9 ms, total: 1.17 s
Wall time: 45.8 s
# building model with best parameters
bagging_tuned4 = BaggingClassifier(
    n_estimators=50,
    max_samples=0.7,
    max_features=0.9,
    bootstrap=False,
    bootstrap_features=False,

)

# Fit the model on training data
bagging_tuned4.fit(X_train_over, y_train_over)

BaggingClassifier
?i
BaggingClassifier(bootstrap=False, max_features=0.9, max_samples=0.7,
                  n_estimators=50)
# Calculating different metrics on train set
bagging_random_train_over = model_performance_classification_sklearn(
    bagging_tuned4, X_train_over, y_train_over
)
print("Training performance:")
bagging_random_train_over
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  95   0   0
 1       1  95   0   0
 2       2  95   0   0
 3       3  95   0   0
 4       4  95   0   0
 5       5  95   0   0
 6       6  95   0   0
 7       7  95   0   0
 8       8  95   0   0
 9       9  95   0   0
 10     10  95   0   0
 11     11  95   0   0
 12     12  95   0   0
 13     13  95   0   0
 14     14  95   0   0
 15     15  95   0   0
 16     16  95   0   0
 17     17  95   0   0
 18     18  95   0   0
 19     19  95   0   0)
# Calculating different metrics on validation set
bagging_random_val_over = model_performance_classification_sklearn(bagging_tuned4, X_val, y_val)
print("Validation performance:")
bagging_random_val_over
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.893   0.893      0.900 0.894,
     Class  TP  FP  FN
 0       0  20  14   7
 1       1  21   5   5
 2       2  26   1   0
 3       3  24   3  12
 4       4  24   1   2
 5       5  35   0   0
 6       6  15   6   6
 7       7  37   2   2
 8       8  33   0   0
 9       9  18  10   7
 10     10  26   2   6
 11     11  20   3   2
 12     12  23   1   0
 13     13  23   2   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   1   1
 17     17  22   1   8
 18     18  29   0   0
 19     19  21   5   0)
Sample tuning method for Random Forest with undersampled data
Random Forest
GridSearchCV

%%time


# Defining model
model = RandomForestClassifier(random_state=1)

# More regularized Parameter grid for Random Forest
param_grid = {
    'n_estimators': [100, 150, 200],         # moderate number of trees
    'max_depth': [5, 10, 15],                # limit depth (shallow trees help generalize)
    'min_samples_split': [5, 10, 15],        # require more samples to split (prevent small splits)
    'min_samples_leaf': [2, 4, 6],           # each leaf has more samples (prevent memorization)
    'max_features': ['sqrt', 'log2'],        # fewer features per split → more randomness
    'bootstrap': [True]                      # bootstrap=True generally reduces variance
}

# Scorer
scorer = metrics.make_scorer(metrics.recall_score, average='macro')  # macro because it's multiclass

# Calling GridSearchCV
grid_cv = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    verbose=2
)

# Fitting
grid_cv.fit(X_train_un, y_train_un)  # <-- IMPORTANT: undersampled data

# Results
print("\nBest parameters are {} with CV score={:.4f}".format(grid_cv.best_params_, grid_cv.best_score_))
Fitting 5 folds for each of 162 candidates, totalling 810 fits

Best parameters are {'bootstrap': True, 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 200} with CV score=0.8945
CPU times: user 5.04 s, sys: 659 ms, total: 5.7 s
Wall time: 5min 38s
# building model with best parameters
rf_tuned5= RandomForestClassifier(
    bootstrap=True,
    max_depth=15,
    max_features='sqrt',
    min_samples_leaf=2,
    min_samples_split=5,
    n_estimators=200,

)

# Fit the model on training data
rf_tuned5.fit(X_train_over, y_train_over)

RandomForestClassifier
?i
RandomForestClassifier(max_depth=15, min_samples_leaf=2, min_samples_split=5,
                       n_estimators=200)
# Calculating different metrics on train set
rf_grid_train_un = model_performance_classification_sklearn(
    rf_tuned5, X_train_un, y_train_un
)
print("Training performance:")
rf_grid_train_un
Training performance:
(   Accuracy  Recall  Precision    F1
 0     0.981   0.981      0.982 0.981,
     Class  TP  FP  FN
 0       0  66   5   1
 1       1  67   1   0
 2       2  67   0   0
 3       3  60   0   7
 4       4  63   1   4
 5       5  67   0   0
 6       6  66   5   1
 7       7  67   0   0
 8       8  67   0   0
 9       9  66   4   1
 10     10  64   2   3
 11     11  66   3   1
 12     12  66   1   1
 13     13  67   2   0
 14     14  67   0   0
 15     15  66   1   1
 16     16  66   0   1
 17     17  64   0   3
 18     18  67   0   0
 19     19  66   0   1)
# Calculating different metrics on validation set
rf_grid_val_un = model_performance_classification_sklearn(rf_tuned5, X_val, y_val)
print("Validation performance:")
rf_grid_val_un
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.890   0.890      0.897 0.889,
     Class  TP  FP  FN
 0       0  18  12   9
 1       1  24   7   2
 2       2  26   1   0
 3       3  21   4  15
 4       4  25   1   1
 5       5  35   0   0
 6       6  15   8   6
 7       7  34   2   5
 8       8  33   0   0
 9       9  21   6   4
 10     10  27   3   5
 11     11  22   9   0
 12     12  23   1   0
 13     13  23   3   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   0   1
 17     17  21   1   9
 18     18  28   0   1
 19     19  19   1   2)
RandomizedSearchCV

%%time

# Define the model
model = RandomForestClassifier(random_state=1)

# Define parameter distribution
param_dist = {
    'n_estimators': [100, 150, 200],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

# Define the scorer
scorer = make_scorer(recall_score, average='macro')

# Define RandomizedSearchCV
randomized_cv = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=10,
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    random_state=1,
    verbose=2
)

# Fit RandomizedSearchCV
randomized_cv.fit(X_train_un, y_train_un)

# Best parameters and best score
print("Best parameters are {} with CV score={:.4f}:".format(
    randomized_cv.best_params_, randomized_cv.best_score_))
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters are {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 30} with CV score=0.8987:
CPU times: user 1.27 s, sys: 126 ms, total: 1.4 s
Wall time: 32.9 s
# building model with best parameters
rf_tuned6 = RandomForestClassifier(
    max_depth=30,
    max_features='sqrt',
    min_samples_leaf=1,
    min_samples_split=2,
    n_estimators=200,

)

# Fit the model on training data
rf_tuned6.fit(X_train_un, y_train_un)

RandomForestClassifier
?i
RandomForestClassifier(max_depth=30, n_estimators=200)
# Calculating different metrics on train set
rf_random_train_un = model_performance_classification_sklearn(
    rf_tuned6, X_train_un, y_train_un
)
print("Training performance:")
rf_random_train_un
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  67   0   0
 1       1  67   0   0
 2       2  67   0   0
 3       3  67   0   0
 4       4  67   0   0
 5       5  67   0   0
 6       6  67   0   0
 7       7  67   0   0
 8       8  67   0   0
 9       9  67   0   0
 10     10  67   0   0
 11     11  67   0   0
 12     12  67   0   0
 13     13  67   0   0
 14     14  67   0   0
 15     15  67   0   0
 16     16  67   0   0
 17     17  67   0   0
 18     18  67   0   0
 19     19  67   0   0)
# Calculating different metrics on validation set
rf_random_val_un = model_performance_classification_sklearn(rf_tuned6, X_val, y_val)
print("Validation performance:")
rf_random_val_un
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.886   0.886      0.888 0.885,
     Class  TP  FP  FN
 0       0  18   8   9
 1       1  21   6   5
 2       2  26   1   0
 3       3  22   7  14
 4       4  23   2   3
 5       5  35   0   0
 6       6  14   7   7
 7       7  36   2   3
 8       8  33   0   0
 9       9  20   9   5
 10     10  28   4   4
 11     11  22   7   0
 12     12  23   1   0
 13     13  23   4   0
 14     14  21   0   0
 15     15  25   0   0
 16     16  23   1   1
 17     17  22   2   8
 18     18  27   0   2
 19     19  20   1   1)
Xgboost
GridSearchCV

%%time

# Defining model
model = XGBClassifier(random_state=1, eval_metric='mlogloss')

# Parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [150, 200, 250],
    'learning_rate': [0.1, 0.2],
    'gamma': [0, 3, 5],
    'subsample': [0.8, 0.9]
}

# Define scoring metric (macro recall for multiclass)
scorer = make_scorer(recall_score, average="macro")

# Calling GridSearchCV
grid_cv = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    verbose=2
)

# Fitting parameters in GridSearchCV (using oversampled data)
grid_cv.fit(X_train_un, y_train_un)

# Print best parameters and best score
print("Best parameters are {} with CV score={}:" .format(grid_cv.best_params_, grid_cv.best_score_))
Fitting 5 folds for each of 36 candidates, totalling 180 fits
Best parameters are {'gamma': 0, 'learning_rate': 0.2, 'n_estimators': 250, 'subsample': 0.9} with CV score=0.8994505494505495:
CPU times: user 3.49 s, sys: 304 ms, total: 3.79 s
Wall time: 1min 49s
# building model with best parameters
xgb_tuned5 = XGBClassifier(
    random_state=1,
    subsample=0.9,
    n_estimators=250,
    learning_rate=0.2,
    gamma=0,
)

# Fit the model on training data
xgb_tuned5.fit(X_train_un, y_train_un)

XGBClassifier
?i
XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=0, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.2, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=250,
              n_jobs=None, num_parallel_tree=None, ...)
# Calculating different metrics on train set
xgboost_grid_train_un = model_performance_classification_sklearn(
    xgb_tuned5, X_train_un, y_train_un
)
print("Training performance:")
xgboost_grid_train_un
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  67   0   0
 1       1  67   0   0
 2       2  67   0   0
 3       3  67   0   0
 4       4  67   0   0
 5       5  67   0   0
 6       6  67   0   0
 7       7  67   0   0
 8       8  67   0   0
 9       9  67   0   0
 10     10  67   0   0
 11     11  67   0   0
 12     12  67   0   0
 13     13  67   0   0
 14     14  67   0   0
 15     15  67   0   0
 16     16  67   0   0
 17     17  67   0   0
 18     18  67   0   0
 19     19  67   0   0)
# Calculating different metrics on validation set
xgboost_grid_val_un = model_performance_classification_sklearn(xgb_tuned5, X_val, y_val)
print("Validation performance:")
xgboost_grid_val_un
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.879   0.879      0.882 0.878,
     Class  TP  FP  FN
 0       0  18  11   9
 1       1  17   4   9
 2       2  26   0   0
 3       3  24  11  12
 4       4  24   4   2
 5       5  35   1   0
 6       6  13   7   8
 7       7  38   2   1
 8       8  33   0   0
 9       9  20  10   5
 10     10  31   2   1
 11     11  16   7   6
 12     12  23   3   0
 13     13  23   2   0
 14     14  21   0   0
 15     15  25   0   0
 16     16  23   0   1
 17     17  24   1   6
 18     18  24   0   5
 19     19  20   1   1)
RandomizedSearchCV

%%time

# Defining the model
model = XGBClassifier(random_state=1, eval_metric='mlogloss')

# Parameter grid to pass in RandomizedSearchCV
param_grid = {
    'n_estimators': [150, 200, 250],
    'scale_pos_weight': [1, 2, 5],  # Adjusted for multiclass
    'learning_rate': [0.1, 0.2],
    'gamma': [0, 3, 5],
    'subsample': [0.8, 0.9]
}

# Defining the scorer (macro recall for multiclass)
scorer = make_scorer(recall_score, average='macro')

# Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=10,
    n_jobs=-1,
    scoring=scorer,
    cv=5,
    random_state=1,
    verbose=2
)

# Fitting parameters in RandomizedSearchCV
randomized_cv.fit(X_train_un, y_train_un)

# Printing best parameters and best score
print("Best parameters are {} with CV score={:.4f}:".format(
    randomized_cv.best_params_, randomized_cv.best_score_))
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters are {'subsample': 0.8, 'scale_pos_weight': 1, 'n_estimators': 150, 'learning_rate': 0.2, 'gamma': 3} with CV score=0.8292:
CPU times: user 1.17 s, sys: 61.5 ms, total: 1.23 s
Wall time: 23.5 s
# building model with best parameters
xgb_tuned6 = XGBClassifier(
    random_state=1,
    subsample=0.8,
    n_estimators=150,
    learning_rate=0.2,
    gamma=3,
    scale_pos_weight=1,
)

# Fit the model on training data
xgb_tuned6.fit(X_train_un, y_train_un)

XGBClassifier
?i
XGBClassifier(base_score=None, booster=None, callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device=None, early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=3, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=0.2, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=150,
              n_jobs=None, num_parallel_tree=None, ...)
# Calculating different metrics on train set
xgboost_random_train_un = model_performance_classification_sklearn(
    xgb_tuned6, X_train_un, y_train_un
)
print("Training performance:")
xgboost_random_train_un
Training performance:
(   Accuracy  Recall  Precision    F1
 0     0.904   0.904      0.908 0.904,
     Class  TP  FP  FN
 0       0  54  25  13
 1       1  65   9   2
 2       2  67   1   0
 3       3  43   6  24
 4       4  55  17  12
 5       5  67   2   0
 6       6  54  18  13
 7       7  62   1   5
 8       8  66   0   1
 9       9  49   5  18
 10     10  58  11   9
 11     11  61  16   6
 12     12  64   2   3
 13     13  67   3   0
 14     14  67   0   0
 15     15  66   1   1
 16     16  64   0   3
 17     17  54   5  13
 18     18  65   0   2
 19     19  63   7   4)
# Calculating different metrics on validation set
xgboost_random_val_un = model_performance_classification_sklearn(xgb_tuned6, X_val, y_val)
print("Validation performance:")
xgboost_random_val_un
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.790   0.790      0.812 0.792,
     Class  TP  FP  FN
 0       0  17  26  10
 1       1  14   3  12
 2       2  26   1   0
 3       3  14   8  22
 4       4  15  10  11
 5       5  34   2   1
 6       6  13  12   8
 7       7  32   6   7
 8       8  32   0   1
 9       9  17   3   8
 10     10  25  10   7
 11     11  15  18   7
 12     12  23   3   0
 13     13  23   7   0
 14     14  21   0   0
 15     15  25   0   0
 16     16  23   0   1
 17     17  19   1  11
 18     18  23   1   6
 19     19  19   3   2)
Bagging
GridSearchCV

%%time

# Defining Bagging model
model = BaggingClassifier(random_state=1)

# Optimized parameter grid for Bagging
param_grid = {
    'n_estimators': [50, 100, 150],        # number of base estimators
    'max_samples': [0.7, 0.8, 1.0],        # % of samples for each estimator
    'max_features': [0.7, 0.8, 1.0],       # % of features for each estimator
    'bootstrap': [True],                   # bootstrap sampling
    'bootstrap_features': [False]          # keep features fixed
}

# Macro recall for balanced evaluation
scorer = metrics.make_scorer(metrics.recall_score, average="macro")

# Optimized GridSearchCV
grid_cv = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring=scorer,
    cv=3,                     # Reduced cv to 3 for speed
    n_jobs=-1,
    verbose=2
)

# Fitting on oversampled data
grid_cv.fit(X_train_un, y_train_un)

# Output best parameters and score
print("Best parameters are {} with CV score={}".format(grid_cv.best_params_, grid_cv.best_score_))
Fitting 3 folds for each of 27 candidates, totalling 81 fits
Best parameters are {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.8, 'max_samples': 1.0, 'n_estimators': 50} with CV score=0.8903820816864295
CPU times: user 793 ms, sys: 140 ms, total: 933 ms
Wall time: 38.2 s
# building model with best parameters
bagging_tuned5 = BaggingClassifier(
    n_estimators=50,
    max_samples=1.0,
    max_features=0.8,
    bootstrap=True,
    bootstrap_features=False,
)
# Fit the model on training data
bagging_tuned5.fit(X_train_un, y_train_un)

BaggingClassifier
?i
BaggingClassifier(max_features=0.8, n_estimators=50)
# Calculating different metrics on train set
bagging_grid_train_un = model_performance_classification_sklearn(
    bagging_tuned5, X_train_un, y_train_un
)
print("Training performance:")
bagging_grid_train_un
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  67   0   0
 1       1  67   0   0
 2       2  67   0   0
 3       3  67   0   0
 4       4  67   0   0
 5       5  67   0   0
 6       6  67   0   0
 7       7  67   0   0
 8       8  67   0   0
 9       9  67   0   0
 10     10  67   0   0
 11     11  67   0   0
 12     12  67   0   0
 13     13  67   0   0
 14     14  67   0   0
 15     15  67   0   0
 16     16  67   0   0
 17     17  67   0   0
 18     18  67   0   0
 19     19  67   0   0)
# Calculating different metrics on validation set
bagging_grid_val_un = model_performance_classification_sklearn(bagging_tuned5, X_val, y_val)
print("Validation performance:")
bagging_grid_val_un
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.860   0.860      0.867 0.860,
     Class  TP  FP  FN
 0       0  15  12  12
 1       1  21   4   5
 2       2  26   1   0
 3       3  19   8  17
 4       4  22   2   4
 5       5  35   1   0
 6       6  16   9   5
 7       7  32   2   7
 8       8  33   0   0
 9       9  17  15   8
 10     10  27   4   5
 11     11  20   8   2
 12     12  23   0   0
 13     13  23   2   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   2   1
 17     17  21   1   9
 18     18  28   0   1
 19     19  21   4   0)
RandomizedSearchCV

%%time

# Defining Bagging model
model = BaggingClassifier(random_state=1)

# Parameter grid for RandomizedSearchCV
param_grid = {
    'n_estimators': [50, 100, 150, 200],      # number of base estimators
    'max_samples': [0.7, 0.8, 0.9, 1.0],      # % of samples
    'max_features': [0.7, 0.8, 0.9, 1.0],     # % of features
    'bootstrap': [True, False],               # bootstrap or not
    'bootstrap_features': [False, True]       # bootstrap features or not
}

# Scoring metric (macro recall)
scorer = metrics.make_scorer(metrics.recall_score, average="macro")

# Calling RandomizedSearchCV
randomized_cv = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_grid,
    n_iter=10,  # number of random combinations
    scoring=scorer,
    cv=5,
    n_jobs=-1,
    random_state=1,
    verbose=2
)

# Fitting on oversampled data
randomized_cv.fit(X_train_un, y_train_un)

# Printing best parameters and score
print("Best parameters are {} with CV score={}".format(randomized_cv.best_params_, randomized_cv.best_score_))
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters are {'n_estimators': 100, 'max_samples': 0.7, 'max_features': 0.8, 'bootstrap_features': False, 'bootstrap': False} with CV score=0.9002197802197802
CPU times: user 1.06 s, sys: 70.2 ms, total: 1.13 s
Wall time: 36.7 s
# building model with best parameters
bagging_tuned5 = BaggingClassifier(
    n_estimators=100,
    max_samples=0.7,
    max_features=0.8,
    bootstrap=False,
    bootstrap_features=False,

)

# Fit the model on training data
bagging_tuned5.fit(X_train_un, y_train_un)

BaggingClassifier
?i
BaggingClassifier(bootstrap=False, max_features=0.8, max_samples=0.7,
                  n_estimators=100)
# Calculating different metrics on train set
bagging_random_train_un = model_performance_classification_sklearn(
    bagging_tuned5, X_train_un, y_train_un
)
print("Training performance:")
bagging_random_train_un
Training performance:
(   Accuracy  Recall  Precision    F1
 0     1.000   1.000      1.000 1.000,
     Class  TP  FP  FN
 0       0  67   0   0
 1       1  67   0   0
 2       2  67   0   0
 3       3  67   0   0
 4       4  67   0   0
 5       5  67   0   0
 6       6  67   0   0
 7       7  67   0   0
 8       8  67   0   0
 9       9  67   0   0
 10     10  67   0   0
 11     11  67   0   0
 12     12  67   0   0
 13     13  67   0   0
 14     14  67   0   0
 15     15  67   0   0
 16     16  67   0   0
 17     17  67   0   0
 18     18  67   0   0
 19     19  67   0   0)
# Calculating different metrics on validation set
bagging_random_val_un = model_performance_classification_sklearn(bagging_tuned5, X_val, y_val)
print("Validation performance:")
bagging_random_val_un
Validation performance:
(   Accuracy  Recall  Precision    F1
 0     0.877   0.877      0.882 0.876,
     Class  TP  FP  FN
 0       0  18  12   9
 1       1  21   4   5
 2       2  26   1   0
 3       3  21   8  15
 4       4  24   1   2
 5       5  35   0   0
 6       6  14   9   7
 7       7  32   2   7
 8       8  33   0   0
 9       9  19  10   6
 10     10  27   4   5
 11     11  21   8   1
 12     12  23   0   0
 13     13  23   2   0
 14     14  21   1   0
 15     15  25   0   0
 16     16  23   1   1
 17     17  21   1   9
 18     18  29   0   0
 19     19  21   3   0)
Model performance comparison and choosing the final model
# Extracting only the performance metrics DataFrame (first element of each tuple)
models_train_comp_df = pd.concat(
    [
        rf_grid_train[0].T,
        rf_random_train[0].T,
        xgboost_grid_train[0].T,
        xgboost_random_train[0].T,
        bagging_grid_train[0].T,
        bagging_random_train[0].T,
        rf_grid_train_over[0].T,
        rf_random_train_over[0].T,
        xgboost_grid_train_over[0].T,
        xgboost_random_train_over[0].T,
        bagging_grid_train_over[0].T,
        bagging_random_train_over[0].T,
        rf_grid_train_un[0].T,
        rf_random_train_un[0].T,
        xgboost_grid_train_un[0].T,
        xgboost_random_train_un[0].T,
        bagging_grid_train_un[0].T,
        bagging_random_train_un[0].T,

       ],
    axis=1,
)

models_train_comp_df.columns = [
    'rf_grid_train',
    'rf_random_train',
    'xgboost_grid_train',
    'xgboost_random_train',
    'bagging_grid_train',
    'bagging_random_train',
    'rf_grid_train_over',
    'rf_random_train_over',
    'xgboost_grid_train_over',
    'xgboost_random_train_over',
    'bagging_grid_train_over',
    'bagging_random_train_over',
    'rf_grid_train_un',
    'rf_random_train_un',
    'xgboost_grid_train_un',
    'xgboost_random_train_un',
    'bagging_grid_train_un',
    'bagging_random_train_un',

]

print("Training performance comparison:")
models_train_comp_df
Training performance comparison:
rf_grid_train	rf_random_train	xgboost_grid_train	xgboost_random_train	bagging_grid_train	bagging_random_train	rf_grid_train_over	rf_random_train_over	xgboost_grid_train_over	xgboost_random_train_over	bagging_grid_train_over	bagging_random_train_over	rf_grid_train_un	rf_random_train_un	xgboost_grid_train_un	xgboost_random_train_un	bagging_grid_train_un	bagging_random_train_un
Accuracy	0.993	1.000	1.000	1.000	1.000	1.000	0.980	1.000	1.000	0.926	1.000	1.000	0.981	1.000	1.000	0.904	1.000	1.000
Recall	0.993	1.000	1.000	1.000	1.000	1.000	0.980	1.000	1.000	0.926	1.000	1.000	0.981	1.000	1.000	0.904	1.000	1.000
Precision	0.993	1.000	1.000	1.000	1.000	1.000	0.980	1.000	1.000	0.926	1.000	1.000	0.982	1.000	1.000	0.908	1.000	1.000
F1	0.993	1.000	1.000	1.000	1.000	1.000	0.980	1.000	1.000	0.926	1.000	1.000	0.981	1.000	1.000	0.904	1.000	1.000
# Validation performance comparison
models_val_comp_df = pd.concat(
    [
        rf_grid_val[0].T,
        rf_random_val[0].T,
        xgboost_grid_val[0].T,
        xgboost_random_val[0].T,
        bagging_grid_val[0].T,
        bagging_random_val[0].T,
        rf_grid_val_over[0].T,
        rf_random_val_over[0].T,
        xgboost_grid_val_over[0].T,
        xgboost_random_val_over[0].T,
        bagging_grid_val_over[0].T,
        bagging_random_val_over[0].T,
        rf_grid_val_un[0].T,
        rf_grid_val_un[0].T,
        xgboost_grid_val_un[0].T,
        xgboost_random_val_un[0].T,
        bagging_grid_val_un[0].T,
        bagging_random_val_un[0].T,

  ],
    axis=1,
)

models_val_comp_df.columns = [
    'rf_grid_val',
    'rf_random_val',
    'xgboost_grid_val',
    'xgboost_random_val',
    'bagging_grid_val',
    'bagging_random_val',
    'rf_grid_val_over',
    'rf_random_val_over',
    'xgboost_grid_val_over',
    'xgboost_random_val_over',
    'bagging_grid_val_over',
    'bagging_random_val_over',
    'rf_grid_val_un',
    'rf_grid_val_un',
    'xgboost_grid_val_un',
    'xgboost_random_val_un',
    'bagging_grid_val_un',
    'bagging_random_val_un',

]

print("Validation performance comparison:")
models_val_comp_df

rf_grid_val	rf_random_val	xgboost_grid_val	xgboost_random_val	bagging_grid_val	bagging_random_val	rf_grid_val_over	rf_random_val_over	xgboost_grid_val_over	xgboost_random_val_over	bagging_grid_val_over	bagging_random_val_over	rf_grid_val_un	rf_grid_val_un	xgboost_grid_val_un	xgboost_random_val_un	bagging_grid_val_un	bagging_random_val_un
Accuracy	0.901	0.897	0.903	0.899	0.860	0.879	0.888	0.906	0.904	0.807	0.884	0.893	0.890	0.890	0.879	0.790	0.860	0.877
Recall	0.901	0.897	0.903	0.899	0.860	0.879	0.888	0.906	0.904	0.807	0.884	0.893	0.890	0.890	0.879	0.790	0.860	0.877
Precision	0.907	0.902	0.908	0.902	0.860	0.884	0.895	0.912	0.909	0.821	0.893	0.900	0.897	0.897	0.882	0.812	0.867	0.882
F1	0.900	0.897	0.902	0.898	0.858	0.878	0.888	0.906	0.904	0.807	0.884	0.894	0.889	0.889	0.878	0.792	0.860	0.876

